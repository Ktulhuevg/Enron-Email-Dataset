{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project : Identify fraud from Enron Email\n",
    "\n",
    "## Project Overview \n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial datas of top enron executives. \n",
    "\n",
    "The Enron datasets comprising emails and financial datas of Enron were made available for public for research and analysis and can be downloaded from : https://www.cs.cmu.edu/~./enron/.\n",
    "\n",
    "### Goal of the project\n",
    "The goal of this project is to use the machine learning skills to build a POI(person of interest) identifier based on financial and email data made public as a result of the Enron scandal. The POI is an acronym for 'person of interest' i.e., a person who is charged by the law for commiting a crime, in this case the scandal at Enron.  \n",
    "\n",
    "The overall work done for this project can be divided into four parts :\n",
    "   1.  __Exploring the Enron Dataset__  \n",
    "       This involves data cleaning, outlier removal and analyzing.\n",
    "\n",
    "   2.  __Feature processing of the Enron dataset__  \n",
    "       It includes feature creation, feature scaling, feature selection and feature transform.\n",
    "      \n",
    "   3.  __Choosing the algorithm__   \n",
    "       As the dataset given has labelled data and output expected is also discrete so supervised learning techniques are used in this project. The supervised learning algorithms are tuned to achieve the best performance on the test dataset. \n",
    "       \n",
    "   4. __Evaluation__  \n",
    "      This step involves validation followed by overall performance check of the project done using evaluation metrics such as precision, recall and f1_score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploring the Enron Dataset\n",
    "\n",
    "The pickled Enron data is loaded as a pandas dataframe for easy anlysis of the dataset. And the key i.e., the Enron employees name is used as the index of the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the given pickled Enron data to a pandas dataframe\n",
    "enron_df = pd.DataFrame.from_records(list(data_dict.values()))\n",
    "\n",
    "# set the index of df to be the employees series:\n",
    "employees = pd.Series(list(data_dict.keys()))\n",
    "enron_df.set_index(employees, inplace=True)\n",
    "enron_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Size of the enron dataframe : \",enron_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Number of data points(people) in the dataset : \",len(enron_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"To find the number of Features in the Enron Dataset : \",len(enron_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counting the number of POIs and non-POIs in the given dataset\n",
    "poi_count = enron_df.groupby('poi').size()\n",
    "print \"Total number of POI's in the given dataset : \",poi_count.iloc[1]\n",
    "print \"Total number of non-POI's in the given dataset : \",poi_t.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you load the data into the dataframe, the data types are strings (or, in pandas, objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the datatypes in the given pandas dataframe into floating points for analysis and replace NaN with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coerce numeric values into floats or ints; also change NaN to zero:\n",
    "enron_df_new = enron_df.apply(lambda x : pd.to_numeric(x, errors = 'coerce')).copy().fillna(0)\n",
    "enron_df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the column of email_address from the enron_df as it is not of much used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping column 'email_address' as not required in analysis\n",
    "enron_df_new.drop('email_address', axis = 1, inplace = True)\n",
    "\n",
    "# Checking the changed shape of df\n",
    "enron_df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Investigation and Analyzing the features of Enron Dataset\n",
    "\n",
    "#### Financial Features : Bonus and Salary\n",
    "Drawing scatterplot of Bonus vs Salary of Enron employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(enron_df_new['salary'][enron_df_new['poi'] == True],enron_df_new['bonus'][enron_df_new['poi'] == True], color = 'r',\n",
    "           label = 'POI')\n",
    "plt.scatter(enron_df_new['salary'][enron_df_new['poi'] == False],enron_df_new['bonus'][enron_df_new['poi'] == False],color = 'b',\n",
    "           label = 'Not-POI')\n",
    "    \n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Bonus\")\n",
    "plt.title(\"Scatterplot of salary vs bonus w.r.t POI\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, one point has very high value of salary and bonus. Checking for the concerned point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding the non-POI employee having maximum salary\n",
    "enron_df_new['salary'].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Outlier 1 : 'TOTAL'\n",
    "The 'TOTAL' row corresponding to the outlier is removed from the enron dataframe. And again the scatterplot is drawn for bonus vs salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting the row 'Total' from the \n",
    "enron_df_new.drop('TOTAL', axis = 0, inplace = True)\n",
    "\n",
    "# Drawing scatterplot with the modified dataframe\n",
    "plt.scatter(enron_df_new['salary'][enron_df_new['poi'] == True],enron_df_new['bonus'][enron_df_new['poi'] == True], color = 'r',\n",
    "           label = 'POI')\n",
    "plt.scatter(enron_df_new['salary'][enron_df_new['poi'] == False],enron_df_new['bonus'][enron_df_new['poi'] == False],color = 'b',\n",
    "           label = 'Not-POI')\n",
    "    \n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Bonus\")\n",
    "plt.title(\"Scatterplot of salary vs bonus w.r.t POI\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, its observed that the data becomes more spread out and more comprehensable after the outlier removal. Its also observed that values of bonuses of POIs are higher than that of non-POIs.\n",
    "\n",
    "As the POI's were taking larger amounts of money as bonus, in addition to their high salary so it can be stated that the ratio of bonus to salary of the POI's will be higher as compared to that of non-POI's. So i create a new feature called __ bonus-to-salary_ratio__ hoping that it may aid in the POI identification in the later parts of this project.\n",
    "\n",
    "#### Feature created 1 : bonus-to-salary_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Created a new feature\n",
    "enron_df_new['bonus-to-salary_ratio'] = enron_df_new['bonus']/enron_df_new['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Outlier 2 : 'THE TRAVEL AGENCY IN THE PARK'\n",
    "\n",
    "From the _enron61702insiderpay.pdf_ provided by findlaw.com, a dataset was observed named 'THE TRAVEL AGENCY IN THE PARK'.\n",
    "From the documentary [Enron The Smartest Guys In The Room](https://www.youtube.com/watch?v=H2f7FunDuTU) i had learnt that Enron had made up some transactions with bogus companies and  people so on observing the features of this dataset it can be considered as an outlier with very low values in all features except in _others_ and _total-payments_ and i am removing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features of the index 'THE TRAVEL AGENCY IN THE PARK'\n",
    "enron_df_new.loc['THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting the row with index 'THE TRAVEL AGENCY IN THE PARK'\n",
    "enron_df_new.drop('THE TRAVEL AGENCY IN THE PARK', axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Financial Features : Deferred_income, deferred_payment and total_payment \n",
    "\n",
    "According to [BusinessDictionary.com](http://www.businessdictionary.com/definition/deferred-payment.html) : Deferred payment is \"a loan arrangement in which the borrower is allowed to start making payments at some specified time in the future. Deferred payment arrangements are often used in retail settings where a person buys and receives an item with a commitment to begin making payments at a future date.\"\n",
    "\n",
    "[Deferred income](https://en.wikipedia.org/wiki/Deferred_income) : (also known as deferred revenue, unearned revenue, or unearned income) is, in accrual accounting, money received for goods or services which have not yet been delivered. According to the revenue recognition principle, it is recorded as a liability until delivery is made, at which time it is converted into revenue.\n",
    "\n",
    "As Enron scam involved a lot of undisclosed assets and cheating public by selling assets to shell companies at end of each month and buying them back at the start of next month to hide the acounting losses so there are chances that lot of deferred revenue by the company was used by the POI's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_df_new['deferred_income'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __deferred_income__ feature has mostly negative values as it is the money which has to be returned by the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding out the integer index locations of POIs and non-POIs\n",
    "poi_rs = []\n",
    "non_poi_rs = []\n",
    "for i in range(len(enron_df_new['poi'])):\n",
    "    if enron_df_new['poi'][i] == True:\n",
    "        poi_rs.append(i+1)\n",
    "    else:\n",
    "        non_poi_rs.append(i+1)\n",
    "\n",
    "print \"length poi list : \",len(poi_rs)\n",
    "print \"length non-poi list : \",len(non_poi_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing scatterplot of Employees with deferred income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(non_poi_rs,\n",
    "            enron_df_new['deferred_income'][enron_df_new['poi'] == False],\n",
    "            color = 'b', label = 'Not-POI')\n",
    "\n",
    "plt.scatter(poi_rs,\n",
    "            enron_df_new['deferred_income'][enron_df_new['poi'] == True],\n",
    "            color = 'r', label = 'POI')\n",
    "\n",
    "    \n",
    "plt.xlabel('Employees')\n",
    "plt.ylabel('deferred_income')\n",
    "plt.title(\"Scatterplot of Employees with deferred income\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above scatterplot is not much helpful in either detecting outliers or finding patterns as some POIs as well as non-POIs have high values of deferred income.   \n",
    "\n",
    "Creating a scatterplot of total_payments vs deferral_payments w.r.t POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scatterplot of total_payments vs deferral_payments w.r.t POI\n",
    "plt.scatter(enron_df_new['total_payments'][enron_df_new['poi'] == False],\n",
    "            enron_df_new['deferral_payments'][enron_df_new['poi'] == False],\n",
    "            color = 'b', label = 'Not-POI')\n",
    "\n",
    "plt.scatter(enron_df_new['total_payments'][enron_df_new['poi'] == True],\n",
    "            enron_df_new['deferral_payments'][enron_df_new['poi'] == True],\n",
    "            color = 'r', label = 'POI')\n",
    "\n",
    "    \n",
    "plt.xlabel('total_payments')\n",
    "plt.ylabel('deferral_payments')\n",
    "plt.title(\"Scatterplot of total_payments vs deferral_payments w.r.t POI\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above scatterplot it can be observed that majority of POIs have very low value of deferral payments as compared to the deferral_payments of non-POIs.  \n",
    "From the above we can observe there are two outliers. The one having high value of total_payments is a POI and the other outlier with high value of deferral payments is a non-POI. I am removing the non-POI outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding the non-POI employee having maximum 'deferral_payments'\n",
    "enron_df_new['deferral_payments'].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Outlier 3 : 'FREVERT MARK A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing the non-POI employee having maximum 'deferral_payments'\n",
    "enron_df_new.drop('FREVERT MARK A', axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Financial Features : 'long_term_incentive'\n",
    "\n",
    "Making a scatterplot to check the long_term_incentive of different Enron employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding out the integer index locations of POIs and non-POIs\n",
    "poi_rs = []\n",
    "non_poi_rs = []\n",
    "for i in range(len(enron_df_new['poi'])):\n",
    "    if enron_df_new['poi'][i] == True:\n",
    "        poi_rs.append(i+1)\n",
    "    else:\n",
    "        non_poi_rs.append(i+1)\n",
    "\n",
    "# Making a scatterplot\n",
    "plt.scatter(non_poi_rs,\n",
    "            enron_df_new['long_term_incentive'][enron_df_new['poi'] == False],\n",
    "            color = 'b', label = 'Not-POI')\n",
    "\n",
    "plt.scatter(poi_rs,\n",
    "            enron_df_new['long_term_incentive'][enron_df_new['poi'] == True],\n",
    "            color = 'r', label = 'POI')\n",
    "\n",
    "    \n",
    "plt.xlabel('Employees')\n",
    "plt.ylabel('long_term_incentive')\n",
    "plt.title(\"Scatterplot of Employee number with long_term_incentive\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From figure, one employee has a very high value of __long_term_incentive__. So considering this point as an outlier and removing it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_df_new['long_term_incentive'].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Removing Outlier 4 : 'MARTIN AMANDA K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_df_new.drop('MARTIN AMANDA K', axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Financial Features : restricted_stock and restricted_stock_deferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatterplot of restricted_stock vs 'restricted_stock_deferred' w.r.t POI\n",
    "\n",
    "plt.scatter(enron_df_new['restricted_stock'][enron_df_new['poi'] == False],\n",
    "            enron_df_new['restricted_stock_deferred'][enron_df_new['poi'] == False],\n",
    "            color = 'b', label = 'Not-POI')\n",
    "\n",
    "plt.scatter(enron_df_new['restricted_stock'][enron_df_new['poi'] == True],\n",
    "            enron_df_new['restricted_stock_deferred'][enron_df_new['poi'] == True],\n",
    "            color = 'r', label = 'POI')\n",
    "\n",
    "    \n",
    "plt.xlabel('restricted_stock')\n",
    "plt.ylabel('restricted_stock_deferred')\n",
    "plt.title(\"Scatterplot of restricted_stock vs 'restricted_stock_deferred' w.r.t POI\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_df_new['restricted_stock_deferred'].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So obtained an outlier in the feature __restricted_stock_deferred__. Also taking a quick look at the values of __restricted_stock_deferred__ most of the values are zeros and the remaining few are negative values. The outlier found here is for the enron employee __BHATNAGAR SANJAY__ who is not a POI and in this analysis i am removing this datapoint hoping that it may aid in classification.  \n",
    "\n",
    "And at the other axis of the graph, the other maximum values are of a POI and a non-POI so no need to remove them. \n",
    "\n",
    "#### Removing Outlier 5 : 'BHATNAGAR SANJAY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_df_new.drop('BHATNAGAR SANJAY', axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Email Features :  from_poi_to_this_person and from_this_person_to_poi\n",
    "\n",
    "Also it can be thought that for doing such a big scam the POI's might have frequent contact between them via E-mails so by checking on the number of e-mails transferred between POIs and an Employee we can be able to guess for the involvement of that person in that scam. So finding the relationship using the mail from and to this person with respect to the POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(enron_df_new['from_poi_to_this_person'][enron_df_new['poi'] == False],\n",
    "            enron_df_new['from_this_person_to_poi'][enron_df_new['poi'] == False],\n",
    "            color = 'b', label = 'Not-POI')\n",
    "\n",
    "plt.scatter(enron_df_new['from_poi_to_this_person'][enron_df_new['poi'] == True],\n",
    "            enron_df_new['from_this_person_to_poi'][enron_df_new['poi'] == True],\n",
    "            color = 'r', label = 'POI')\n",
    "\n",
    "    \n",
    "plt.xlabel('from_poi_to_this_person')\n",
    "plt.ylabel('from_this_person_to_poi')\n",
    "plt.title(\"Scatterplot of count of from and to mails between poi and this_person w.r.t POI\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatterplot shows relationship between the count of mails send to and fro among different employees of Enron. I think a different feature showing the proportion of mail sent by employees to and fro to the POI will be more helpful in finding out the POI. As POIs will have more communications with POIs as compared to communication with other non-POIS.   \n",
    "So creating two new features.  \n",
    "\n",
    "#### Features created : fraction_mail_from_poi and fraction_mail_to_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_df_new['fraction_mail_from_poi'] = enron_df_new['from_poi_to_this_person']/enron_df_new['from_messages'] \n",
    "enron_df_new['fraction_mail_to_poi'] = enron_df_new['from_this_person_to_poi']/enron_df_new['to_messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatterplot of fraction of mails from and to between poi and this_person w.r.t POI\n",
    "plt.scatter(enron_df_new['fraction_mail_from_poi'][enron_df_new['poi'] == False],\n",
    "            enron_df_new['fraction_mail_to_poi'][enron_df_new['poi'] == False],\n",
    "            color = 'b', label = 'Not-POI')\n",
    "\n",
    "plt.scatter(enron_df_new['fraction_mail_from_poi'][enron_df_new['poi'] == True],\n",
    "            enron_df_new['fraction_mail_to_poi'][enron_df_new['poi'] == True],\n",
    "            color = 'r', label = 'POI')\n",
    "\n",
    "    \n",
    "plt.xlabel('fraction_mail_from_poi')\n",
    "plt.ylabel('fraction_mail_to_poi')\n",
    "plt.title(\"Scatterplot of fraction of mails from and to between poi and this_person w.r.t POI\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, the difference between POIs and non-POIs points can be clearly classified.  As the red dots representing POIs are more distinct, have higher values and are more seperate from the non-POI blue points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean all 'inf' values which we got if the person's from_messages = 0\n",
    "enron_df_new = enron_df_new.replace('inf', 0)\n",
    "enron_df_new = enron_df_new.fillna(0)\n",
    "# Converting the above modified dataframe to a dictionary\n",
    "enron_dict = enron_df_new.to_dict('index')\n",
    "print \"Features of modified data_dictionary :\"\n",
    "print \"Total number of datapoints : \",len(enron_dict)\n",
    "print \"Total number of features : \",len(enron_dict['METTS MARK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = enron_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features choosen by me to be used in the POI identifier\n",
    "\n",
    "Out of the 23 features available to be me, i will be using 19 of them for the POI identification and feature processing :\n",
    "  - __12 financial features :__ \n",
    "  ['salary', 'bonus', 'long_term_incentive', 'bonus-to-salary_ratio', 'deferral_payments', 'expenses','restricted_stock_deferred', 'restricted_stock', 'deferred_income', 'total_payments','other']\n",
    "  - __6 Email features :__ ['fraction_mail_from_poi', 'fraction_mail_to_poi', 'from_poi_to_this_person', 'from_this_person_to_poi', 'to_messages', 'from_messages']\n",
    "  - __POI__\n",
    "  \n",
    "The features choosen for further processing were mainly dependant on the graphical analysis done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Selecting features which i think might be important\n",
    "features_list = ['poi', 'salary', 'bonus', 'long_term_incentive', 'bonus-to-salary_ratio', 'deferral_payments', 'expenses',\n",
    "                 'restricted_stock_deferred', 'restricted_stock', 'deferred_income','fraction_mail_from_poi', 'total_payments',\n",
    "                 'other', 'fraction_mail_to_poi', 'from_poi_to_this_person', 'from_this_person_to_poi', 'to_messages', \n",
    "                 'from_messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature processing of the Enron dataset\n",
    "\n",
    "Steps involved :  \n",
    "__1. Feature scaling :__ though standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one. Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.  \n",
    "\n",
    "In this project i will be using __MinMaxScaler__ which scales features to lie between zero and one. MinMaxScaler transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.\n",
    "\n",
    "\n",
    "__2. Feature Selection :__ feature selection/dimensionality reduction on sample sets is essential to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets. Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator.   \n",
    "\n",
    "In this project i will be using __SelectKBest__ function to find the K best or high-scoring features. Objects of these function take as input a scoring function that returns univariate scores and p-values. Here __f_classif__ is used as scoring function. The f_classif function computes the ANOVA F-value between labels and features for classification tasks.  \n",
    "\n",
    "\n",
    "__3. Dimensionality Reduction using PCA :__ PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, PCA is implemented as a transformer object that learns n components in its fit method, and can be used on new data to project it on these components.\n",
    "\n",
    "\n",
    "__Pipeline__ is used to sequentially apply feature processing steps such as scaling, selection and classifcation.\n",
    "\n",
    "Machine learning aims to obtain the parameter values that gives the optimal performance. Sklearn's __GridSearchCV__ module automates this process by performing a grid search over a range of parameter values for an estimator.\n",
    "\n",
    "__StratifiedShuffleSplit__ is used as the cross-validation method. 'GridSearchCV' will fit and validate over multiple test/train splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### split data into training and testing datasets\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.3, \n",
    "                                                                                             random_state=42)\n",
    "\n",
    "# Stratified ShuffleSplit cross-validator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# Dont use 'labels' in the input of StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3,random_state = 42)\n",
    "\n",
    "# Importing modules for feature scaling and selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining features to be used via the pipeline\n",
    "## 1. Feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "## 2. Feature Selection\n",
    "skb = SelectKBest(f_classif)\n",
    "\n",
    "## 3. Dimensionality Reduction using PCA\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Choosing Algorithm\n",
    "\n",
    "For this project i used the following algorithms : \n",
    "1. Gaussian Naive Bayes (GaussianNB) classifier\n",
    "2. Decision Tree classifier\n",
    "3. SVM classifier\n",
    "4. KNN classifier\n",
    "\n",
    "#### Table showing the accuracy, precision, recall and f1-score of the algorithms used above\n",
    "\n",
    "| Algorithm                | Accuracy | Precision | Recall | f1 score |\n",
    "|--------------------------|:--------:|----------:|--------|----------|\n",
    "| GaussianNB classifier    | 0.762    | 0.4       | 0.222  | 0.286    |\n",
    "| Decision Tree classifier | 0.785    | 0.4       | 0.25   | 0.307    |\n",
    "| SVM classifier           | 0.785    | 0.2       | 0.167  | 0.182    |\n",
    "| KNN classifier           | 0.857    | 0.6       | 0.428  | 0.5      |\n",
    "\n",
    "\n",
    "As KNN algorithm better precision and recall value as compared to other algorithms so i have choosen it for this project. The code below is for KNN classifier designed for this project. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## KNN classifier code\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_knn = KNeighborsClassifier()\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps = [(\"scaling\", scaler), (\"SKB\", skb), (\"PCA\",pca), (\"knn\",clf_knn)])\n",
    "# Define the parameters to be passsed to GridSearchCV\n",
    "param_grid = {\"SKB__k\":[7,8,9,10,11,12], \n",
    "              \"PCA__n_components\":[2,3,4,5,6,7],\n",
    "              \"PCA__whiten\":[True],\n",
    "              \"knn__n_neighbors\": [3,5,7,9,11]\n",
    "              }\n",
    "\n",
    "# Create GeidSearchCV object\n",
    "clf = GridSearchCV(pipeline, param_grid, verbose = 0, cv = sss, scoring = 'f1')\n",
    "\n",
    "t0 = time()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "print \"training time: \", round(time()-t0, 3), \"s\"\n",
    "\n",
    "t0 = time()\n",
    "prediction = clf.predict(features_test)\n",
    "print \"testing time: \", round(time()-t0, 3), \"s\"\n",
    "\n",
    "print \"Accuracy of KNN classifer is  : \",accuracy_score(labels_test, prediction)\n",
    "print \"Precision of KNN classifer is : \",precision_score(prediction, labels_test)\n",
    "print \"Recall of KNN classifer is    : \",recall_score(prediction, labels_test)\n",
    "print \"f1-score of KNN classifer is  : \",f1_score(prediction, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding out the features selected by SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtaining the boolean list showing selected features\n",
    "features_selected_bool = clf.best_estimator_.named_steps['SKB'].get_support()\n",
    "# Finding the features selected by SelectKBest\n",
    "features_selected_list = [x for x,y in zip(features_list[1:], features_selected_bool) if y]\n",
    "\n",
    "print \"Total number of features selected by SelectKBest algorithm : \",len(features_selected_list)\n",
    "\n",
    "# Finding the score of features \n",
    "feature_scores =  clf.best_estimator_.named_steps['SKB'].scores_\n",
    "# Finding the score of features selected by selectKBest\n",
    "feature_selected_scores = feature_scores[features_selected_bool]\n",
    "\n",
    "# Creating a pandas dataframe and arranging the features based on their scores and rankimg them \n",
    "imp_features_df = pd.DataFrame({'Features_Selected':features_selected_list, 'Features_score':feature_selected_scores})\n",
    "imp_features_df.sort_values('Features_score', ascending = False,inplace = True)\n",
    "Rank = pd.Series(list(range(1,12)))\n",
    "imp_features_df.set_index(Rank, inplace = True)\n",
    "imp_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Tuning the Algorithm \n",
    "Tuning the algorithm for machine learning means to choose the best parameter values for algorithm that provides an optimized performance. If not tuned correctly the decision boundary made using the algorithm and training set data won't be able to correctly give the best prediction for test set data as chances of having high bias and high variance occurs.\n",
    "\n",
    "__Grid search__ is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.  \n",
    "\n",
    "After applying GridSearch along with other feature processing techniques i found that KNN algorithm gives the best prediction, recall and f1 scores.  \n",
    "\n",
    "I also tried to find out Accuracy, precision, recall and f1 score for different values of parameter __n_neighbors__ manually:\n",
    "\n",
    "| n_neighbors | Accuracy | Precision | Recall | f1 score |\n",
    "|-------------|:--------:|----------:|--------|----------|\n",
    "| 3           | 0.857    | 0.6       | 0.428  | 0.5      |\n",
    "| 4           | 0.857    | 0.2       | 0.333  | 0.25     |\n",
    "| 6           | 0.881    | 0.2       | 0.5    | 0.286    |\n",
    "| 9           | 0.881    | 0.2       | 0.5    | 0.286    |\n",
    "| 11          | 0.881    | 0.2       | 0.5    | 0.286    |\n",
    "\n",
    "So for __n_neighbors = 3__, best performance of the algorithm is achieved. \n",
    "\n",
    "Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data or the features selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimator chosen by the grid search\n",
    "# best algorithm-tune combination selected for final analysis\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter setting that gave the best results on the hold out data.\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Validation\n",
    "\n",
    "__Model validation__ is defined as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model. Model validation is carried out after model training.  \n",
    "\n",
    "Training the prediction model and testing it on the same dataset used for testing is a classic mistake. A model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set.  \n",
    "\n",
    "In this project for validation, [StratifiedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html) is used because the dataset used is very small. This method does a \"Random permutation cross-validatoion\".\n",
    "\n",
    "\n",
    "#### Evaluation Metric\n",
    "\n",
    "In this project, i am trying to optimize the precision and recall so i am using the f1-score as the key measure for algorithms's performance. F1 score considers both the precision and the recall to compute the score.\n",
    "\n",
    "- The equation for precision is :  \n",
    "\n",
    "$ precision = (\\frac{TP}{TP + FP}) $\n",
    "\n",
    "- The equation for recall is :\n",
    "\n",
    "$ recall = (\\frac{TP}{TP + FN}) $ \n",
    "\n",
    "- The equation for f1 is :  \n",
    "\n",
    "$ f1 = 2(\\frac{precision.recall}{precision+recall}) $  \n",
    "\n",
    "\n",
    "| Algorithm used | Precision | Recall | f1 score |\n",
    "|:--------------:|----------:|--------|----------|\n",
    "| KNN            | 0.6       | 0.428  | 0.5      |\n",
    "\n",
    ">  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.\n",
    "\n",
    "- Precision can be interpreted as :  if a person is being classified as a POI by my classifier then there is a 60% chance that the person is actually a POI. (i.e., a 60% chance of obtaining a true positive condition.)  \n",
    "\n",
    "- Recall  can be interpreted as : of all the POIs considered, 42.8% of all the POIs can be classified correctly as POI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References :\n",
    "1. [Documentation of scikit-learn 0.19.1](http://scikit-learn.org/stable/documentation.html)\n",
    "2. Udacity Forum\n",
    "3. [Notes on Machine Learning & Artificial Intelligence](https://chrisalbon.com/)\n",
    "4. [Model Validation, Machine Learning](https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-9863-7_233)\n",
    "5. [f1-score](https://en.wikipedia.org/wiki/F1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
